{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_usage(df, name):\n",
    "    mb = df.memory_usage().sum() / 1024 / 1024\n",
    "    print(name + ' use {:.2f} mb in memory'.format(mb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "* Попробовать всё-таки считать прогресс оконной функцией\n",
    "* Погенерировать ещё признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/.DS_Store\n",
      "./data/riiid-test-answer-prediction/lectures.csv\n",
      "./data/riiid-test-answer-prediction/example_sample_submission.csv\n",
      "./data/riiid-test-answer-prediction/questions.csv\n",
      "./data/riiid-test-answer-prediction/train.csv\n",
      "./data/riiid-test-answer-prediction/example_test.csv\n",
      "./data/riiid-test-answer-prediction/riiideducation/competition.cpython-37m-x86_64-linux-gnu.so\n",
      "./data/riiid-test-answer-prediction/riiideducation/__init__.py\n"
     ]
    }
   ],
   "source": [
    "data_path = './data'\n",
    "#data_path = '/kaggle/input'\n",
    "for dirname, _, filenames in os.walk(data_path):\n",
    "    for filename in filenames:\n",
    "        current_file_path = os.path.join(dirname, filename)\n",
    "        if 'lectures.csv' in current_file_path:\n",
    "            lectures_path = current_file_path\n",
    "        if 'questions.csv' in current_file_path:\n",
    "            questions_path = current_file_path\n",
    "        if 'train.csv' in current_file_path:\n",
    "            data_path = current_file_path\n",
    "        if 'example_test.csv' in current_file_path:\n",
    "            test_data_path = current_file_path\n",
    "        print(current_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lectures use 0.01 mb in memory\n",
      "questions use 0.52 mb in memory\n",
      "train use 944.14 mb in memory\n",
      "test use 0.01 mb in memory\n",
      "CPU times: user 27.3 s, sys: 3.61 s, total: 30.9 s\n",
      "Wall time: 30.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lectures_df = pd.read_csv(lectures_path)\n",
    "questions_df = pd.read_csv(questions_path)\n",
    "data = pd.read_csv(data_path, low_memory=False, nrows=3e7, \n",
    "                   dtype={'row_id': 'int64', 'timestamp': 'int64', 'user_id': 'int32', \n",
    "                          'content_id': 'int16', 'content_type_id': 'int8', 'task_container_id': 'int16', \n",
    "                          'user_answer': 'int8', 'answered_correctly': 'int8', \n",
    "                          'prior_question_elapsed_time': 'float32', \n",
    "                          'prior_question_had_explanation': 'boolean'\n",
    "                         }\n",
    "                  )\n",
    "test = pd.read_csv(test_data_path, \n",
    "                   dtype={'row_id': 'int64', 'group_num': 'int64', 'timestamp': 'int64', 'user_id': 'int32', \n",
    "                          'content_id': 'int16', 'content_type_id': 'int8', 'task_container_id': 'int16', \n",
    "                          'user_answer': 'int8', 'answered_correctly': 'int8', \n",
    "                          'prior_question_elapsed_time': 'float32', \n",
    "                          'prior_question_had_explanation': 'boolean',\n",
    "                          'prior_group_answers_correct': 'object', \n",
    "                          'prior_group_responses': 'object'\n",
    "                         }\n",
    "                  )\n",
    "memory_usage(lectures_df, 'lectures')\n",
    "memory_usage(questions_df, 'questions')\n",
    "memory_usage(data, 'train')\n",
    "memory_usage(test, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Формирование массива"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(data, questions_df, lectures_df=None, data_type='train'):\n",
    "    # Собираем общий массив\n",
    "    questions_df.columns = ['content_id', 'bundle_id', 'correct_answer', 'part', 'tags']\n",
    "    if lectures_df is None:\n",
    "        full_df = (\n",
    "            data[data['content_type_id'] != 1]\n",
    "            .merge(questions_df, on='content_id', how='left')\n",
    "        )\n",
    "    else:\n",
    "        lectures_df.columns = ['content_id', 'lecture_tag', 'lecture_part', 'type_of_lecture']\n",
    "        full_df = (\n",
    "            data\n",
    "            .merge(questions_df, on='content_id', how='left')\n",
    "            .merge(lectures_df, on='content_id', how='left')\n",
    "        )\n",
    "        # Уменьшаем размер массива путём объединения колонок и заполенения наллов\n",
    "        full_df.loc[full_df['part'].isnull(), 'part'] = full_df.loc[full_df['part'].isnull(), 'lecture_part']\n",
    "        full_df.loc[full_df['tags'].isnull(), 'tags'] = full_df.loc[full_df['tags'].isnull(), 'lecture_tag'].astype(str)\n",
    "        full_df.drop(columns=['lecture_part', 'lecture_tag'], inplace=True)\n",
    "        full_df.loc[full_df['correct_answer'].isnull(), 'correct_answer'] = -1\n",
    "    \n",
    "    # Приводим типы\n",
    "    full_df['part'] = full_df['part'].astype('int8')\n",
    "    \n",
    "    # Если обучающая выборка, то можем проверить правильность ответа\n",
    "    if data_type == 'train':\n",
    "        full_df['correct_answer'] = full_df['correct_answer'].astype('int8')\n",
    "        # Убеждаемся в правильности ответов\n",
    "        full_df['answered_correctly_really'] = (full_df['user_answer'] == full_df['correct_answer']).astype('int8')\n",
    "        index_dev = (\n",
    "            (full_df['answered_correctly_really'] != full_df['answered_correctly']) \n",
    "            & (full_df['answered_correctly'] != -1)\n",
    "        )\n",
    "        deviations_count = index_dev.sum()\n",
    "        if deviations_count > 0:\n",
    "            print('Wrong answer_correctly in {} rows'.format(deviations_count))\n",
    "            full_df.loc[index_dev, 'answered_correctly'] = full_df.loc[index_dev, 'answered_correctly_really']\n",
    "        full_df.drop(columns=['answered_correctly_really', 'correct_answer'], inplace=True)\n",
    "    \n",
    "    memory_usage(full_df, 'merged_df')\n",
    "    \n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_df = merge_data(data, questions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_question_time(full_df):\n",
    "    # Время, затраченное на ответ (у пачки вопросов временные характеристики одинаковые)\n",
    "    bundles = full_df[['timestamp', 'user_id', 'bundle_id', 'prior_question_elapsed_time']].drop_duplicates()\n",
    "    bundles['question_elapsed_time'] = (\n",
    "        bundles\n",
    "        .sort_values(by=['timestamp'], ascending=True)\n",
    "        .groupby(['user_id'])['prior_question_elapsed_time'].shift(-1)\n",
    "        .dropna()\n",
    "    )\n",
    "    # Добавляем данные к итоговому массиву\n",
    "    full_df = full_df.merge(bundles[['timestamp', 'user_id', 'bundle_id', 'question_elapsed_time']], \n",
    "                            on=['user_id', 'bundle_id', 'timestamp'],\n",
    "                            how='left')\n",
    "    \n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_df = current_question_time(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сбор статистик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions_characteristics(full_df, questions_characteristics=None):\n",
    "    # Время, затраченное на ответ\n",
    "    bundles = (\n",
    "        full_df.groupby('bundle_id').\n",
    "        agg({'question_elapsed_time': ['sum', 'count']}).\n",
    "        reset_index()\n",
    "    )\n",
    "    bundles.columns = ['bundle_id', 'question_bundle_time_sum', 'question_bundle_count']\n",
    "    \n",
    "    if questions_characteristics is None:\n",
    "        # Характеристики правильности ответов\n",
    "        questions_characteristics = (\n",
    "            full_df\n",
    "            .groupby(['content_id', 'bundle_id'])['answered_correctly']\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "        )\n",
    "        questions_characteristics.columns = ['content_id', 'bundle_id', 'question_answered_correctly_mean']\n",
    "        # Итоговый массив харастеристик вопросов\n",
    "        questions_characteristics = questions_characteristics.merge(bundles, on='bundle_id', how='left')\n",
    "    else:\n",
    "        columns = questions_characteristics.columns\n",
    "        columns_update = ['question_bundle_time_sum', 'question_bundle_count']\n",
    "        columns_renamed = [col + '_prev' if col in columns_update else col for col in columns]\n",
    "        questions_characteristics.columns = columns_renamed\n",
    "        questions_characteristics = (\n",
    "            questions_characteristics\n",
    "            .merge(bundles, on=['bundle_id'], how='left')\n",
    "            .fillna(0.)\n",
    "        )\n",
    "        for col in columns_update:\n",
    "            questions_characteristics.loc[:, col] += questions_characteristics.loc[:, col + '_prev']\n",
    "        \n",
    "        questions_characteristics = questions_characteristics[columns]\n",
    "    \n",
    "    questions_characteristics['question_bundle_time_mean'] = questions_characteristics['question_bundle_time_sum'] / questions_characteristics['question_bundle_count']\n",
    "    memory_usage(questions_characteristics, 'questions_characteristics')\n",
    "    \n",
    "    return questions_characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questions_characteristics = get_questions_characteristics(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questions_characteristics.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_users_characteristics(full_df, users_characteristics=None):\n",
    "    # Время, затраченное на ответ\n",
    "    users_time_stats = (\n",
    "        full_df[full_df['content_type_id'] != 1].\n",
    "        groupby('user_id').\n",
    "        agg({'prior_question_elapsed_time': ['sum', 'count']}).\n",
    "        reset_index()\n",
    "    )\n",
    "    users_time_stats.columns = ['user_id', 'user_elapsed_time_sum', \n",
    "                                'user_answers_count']\n",
    "    if users_characteristics is None:\n",
    "        # Характеристики правильности ответов\n",
    "        users_characteristics = (\n",
    "            full_df\n",
    "            .groupby(['user_id'])['answered_correctly']\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "        )\n",
    "        users_characteristics.columns = ['user_id', 'user_answered_correctly_mean']\n",
    "        users_characteristics = users_characteristics.merge(users_time_stats, on='user_id', how='left')\n",
    "    else:\n",
    "        columns = users_characteristics.columns\n",
    "        columns_update = ['user_elapsed_time_sum', 'user_answers_count']\n",
    "        columns_renamed = [col + '_prev' if col in columns_update else col for col in columns]\n",
    "        users_characteristics.columns = columns_renamed\n",
    "        users_characteristics = (\n",
    "            users_characteristics\n",
    "            .merge(users_time_stats, on=['user_id'], how='outer')\n",
    "        )\n",
    "        index_answers_null = users_characteristics['user_answered_correctly_mean'].isnull()\n",
    "        users_characteristics = users_characteristics.fillna(0.)\n",
    "        \n",
    "        for col in columns_update:\n",
    "            users_characteristics.loc[:, col] += users_characteristics.loc[:, col + '_prev']\n",
    "            \n",
    "        users_characteristics = users_characteristics[columns]\n",
    "        # Возвращаем наллы в ответы\n",
    "        users_characteristics.loc[index_answers_null, 'user_answered_correctly_mean'] = np.nan\n",
    "        \n",
    "    users_characteristics['user_elapsed_time_mean'] = users_characteristics['user_elapsed_time_sum'] / users_characteristics['user_answers_count']\n",
    "    \n",
    "    # Заполняем наллы средними значениями\n",
    "    index_null = users_characteristics['user_elapsed_time_mean'].isnull()\n",
    "    users_mean = users_characteristics['user_elapsed_time_mean'].mean()\n",
    "    users_characteristics.loc[index_null, 'user_elapsed_time_mean'] = users_mean\n",
    "    \n",
    "    memory_usage(users_characteristics, 'users_characteristics')\n",
    "    \n",
    "    return users_characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#users_characteristics = get_users_characteristics(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#users_characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_df(full_df, users_characteristics, questions_characteristics):\n",
    "    output = (\n",
    "        full_df\n",
    "        .merge(questions_characteristics[['content_id', 'question_answered_correctly_mean', \n",
    "                                         'question_bundle_time_mean']], on='content_id', how='left')\n",
    "        .merge(users_characteristics[['user_id', 'user_answered_correctly_mean', \n",
    "                                      'user_elapsed_time_mean']], on='user_id', how='left')\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Заполняем наллы первых ответов\n",
    "    index_nulls = output.prior_question_elapsed_time.isnull()\n",
    "    output.loc[index_nulls, 'prior_question_elapsed_time'] = output.loc[index_nulls, 'user_elapsed_time_mean']\n",
    "    output.loc[index_nulls, 'prior_question_had_explanation'] = False\n",
    "    index_nulls = output.prior_question_elapsed_time.isnull()\n",
    "    output.loc[index_nulls, 'prior_question_elapsed_time'] = output.loc[index_nulls, 'question_bundle_time_mean']\n",
    "    \n",
    "    # Заполняем наллы статистик\n",
    "    mean_question_ans = questions_characteristics['question_answered_correctly_mean'].mean()\n",
    "    mean_question_time = questions_characteristics['question_bundle_time_mean'].mean()\n",
    "    mean_user_time = users_characteristics['user_elapsed_time_mean'].mean()\n",
    "    index_nulls = output.question_answered_correctly_mean.isnull()\n",
    "    output.loc[index_nulls, 'question_answered_correctly_mean'] = mean_question_ans\n",
    "    index_nulls = output.question_bundle_time_mean.isnull()\n",
    "    output.loc[index_nulls, 'question_bundle_time_mean'] = mean_question_time\n",
    "    index_nulls = output.user_elapsed_time_mean.isnull()\n",
    "    output.loc[index_nulls, 'user_elapsed_time_mean'] = mean_user_time\n",
    "    \n",
    "    index_nulls = output.user_answered_correctly_mean.isnull()\n",
    "    output.loc[index_nulls, 'user_answered_correctly_mean'] = output.loc[index_nulls, 'question_answered_correctly_mean']\n",
    "    \n",
    "    memory_usage(output, 'combined_df')\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df = get_combined_df(full_df, users_characteristics, questions_characteristics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка обучающей выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train(train_data, questions_df):\n",
    "    # Обработка входных данных\n",
    "    full_df = merge_data(train_data, questions_df)\n",
    "    full_df = current_question_time(full_df)\n",
    "    # Расчёт статистик\n",
    "    questions_characteristics = get_questions_characteristics(full_df)\n",
    "    users_characteristics = get_users_characteristics(full_df)\n",
    "    # Получение итогового датафрейма\n",
    "    full_df = get_combined_df(full_df, users_characteristics, questions_characteristics)\n",
    "    # Формирование обучающей выборки\n",
    "    train_df = (\n",
    "        full_df[['row_id', 'prior_question_elapsed_time', 'prior_question_had_explanation', \n",
    "                 'answered_correctly', 'question_answered_correctly_mean', 'question_bundle_time_mean',\n",
    "                 'user_answered_correctly_mean', 'user_elapsed_time_mean']]\n",
    "        .set_index('row_id')\n",
    "    )\n",
    "    train_df.loc[:, 'prior_question_had_explanation'] = train_df.loc[:, 'prior_question_had_explanation'].astype('int8')\n",
    "    memory_usage(train_df, 'train_df')\n",
    "    \n",
    "    return train_df, questions_characteristics, users_characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df, questions_characteristics, users_characteristics = prepare_train(data, questions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_users_last_answers(data):\n",
    "    users_last_timestamps = data[data['content_type_id'] == 0].groupby('user_id')['timestamp'].max().reset_index()\n",
    "    merge_cols = ['user_id', 'timestamp']\n",
    "    users_last_timestamps.columns = merge_cols\n",
    "    output = data.merge(users_last_timestamps, on=merge_cols, how='inner')\n",
    "    output['old_data'] = True\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#last_users_answers = get_users_last_answers(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test(test_data, last_users_answers, questions_df, users_characteristics, questions_characteristics):\n",
    "    # Обработка входных данных\n",
    "    test_data['old_data'] = False\n",
    "    test_data = pd.concat([last_users_answers, test_data], axis=0)\n",
    "    full_df = merge_data(test_data, questions_df, data_type='test')\n",
    "    full_df = current_question_time(full_df)\n",
    "    full_df = full_df[~full_df['old_data']]\n",
    "    # Обновление статистик\n",
    "    questions_characteristics = get_questions_characteristics(full_df, questions_characteristics)\n",
    "    users_characteristics = get_users_characteristics(full_df, users_characteristics)\n",
    "    # Получение итогового датафрейма\n",
    "    full_df = get_combined_df(full_df, users_characteristics, questions_characteristics)\n",
    "    # Формирование тестовой выборки\n",
    "    test_df = (\n",
    "        full_df.loc[:, \n",
    "                    ['row_id', 'prior_question_elapsed_time', 'prior_question_had_explanation', \n",
    "                     'question_answered_correctly_mean', 'question_bundle_time_mean',\n",
    "                     'user_answered_correctly_mean', 'user_elapsed_time_mean']]\n",
    "        .set_index('row_id')\n",
    "    )\n",
    "    test_df.loc[:, 'prior_question_had_explanation'] = test_df.loc[:, 'prior_question_had_explanation'].astype('int8')\n",
    "    memory_usage(test_df, 'test_df')\n",
    "    \n",
    "    last_users_answers = get_users_last_answers(full_df)\n",
    "    memory_usage(last_users_answers, 'last_users_answers')\n",
    "    \n",
    "    return test_df, questions_characteristics, users_characteristics, last_users_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df, questions_characteristics, users_characteristics, last_users_answers = prepare_test(test, last_users_answers, questions_df, users_characteristics, questions_characteristics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged_df use 1627.16 mb in memory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Готовим исходные данные\n",
    "data = data[data['content_type_id'] == 0]\n",
    "merged_df = merge_data(data, questions_df)\n",
    "last_users_answers = get_users_last_answers(merged_df)\n",
    "del merged_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/n-surkov/opt/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn import metrics\n",
    "import lightgbm as lgbm\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Общие параметры\n",
    "# -----------\n",
    "random_state=13\n",
    "test_size=0.3\n",
    "n_splits = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=test_size, shuffle=False, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged_df use 1139.01 mb in memory\n",
      "questions_characteristics use 0.62 mb in memory\n",
      "users_characteristics use 3.13 mb in memory\n",
      "combined_df use 1688.88 mb in memory\n",
      "train_df use 746.25 mb in memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/n-surkov/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged_df use 572.82 mb in memory\n",
      "questions_characteristics use 0.67 mb in memory\n",
      "users_characteristics use 4.92 mb in memory\n",
      "combined_df use 866.88 mb in memory\n",
      "test_df use 378.73 mb in memory\n",
      "last_users_answers use 5.86 mb in memory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, q_chars, u_chars = prepare_train(train_data, questions_df)\n",
    "last_answers = get_users_last_answers(train_data)\n",
    "test_df, q_chars, u_chars, last_answers = prepare_test(test_data, \n",
    "                                                       last_answers, questions_df, \n",
    "                                                       u_chars, q_chars)\n",
    "test_df = (\n",
    "        test_df.reset_index()\n",
    "        .merge(test_data[['row_id', 'answered_correctly']], on='row_id', how='inner')\n",
    "        .set_index('row_id')\n",
    "    )\n",
    "del q_chars, u_chars, last_answers\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=13)\n",
    "\n",
    "oof = np.zeros(len(train_data))\n",
    "predictions = np.zeros(len(test_data))\n",
    "\n",
    "skf_split = skf.split(X=train_data, y=train_data['answered_correctly'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "        'num_leaves': 80,\n",
    "        'max_bin': 250,\n",
    "        'min_data_in_leaf': 11,\n",
    "        'learning_rate': 0.01,\n",
    "        'min_sum_hessian_in_leaf': 0.00245,\n",
    "        'bagging_fraction': 1.0, \n",
    "        'bagging_freq': 5, \n",
    "        'feature_fraction': 0.05,\n",
    "        'lambda_l1': 4.972,\n",
    "        'lambda_l2': 2.276,\n",
    "        'min_gain_to_split': 0.65,\n",
    "        'max_depth': 14,\n",
    "        'save_binary': True,\n",
    "        'seed': 1337,\n",
    "        'feature_fraction_seed': 1337,\n",
    "        'bagging_seed': 1337,\n",
    "        'drop_seed': 1337,\n",
    "        'data_random_seed': 1337,\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbose': 1,\n",
    "        'metric': 'auc',\n",
    "        'is_unbalance': True,\n",
    "        'boost_from_average': False,\n",
    "        'device': 'cpu',\n",
    "        'gpu_platform_id': 0,\n",
    "        'gpu_device_id': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged_df use 569.50 mb in memory\n",
      "questions_characteristics use 0.62 mb in memory\n",
      "users_characteristics use 3.13 mb in memory\n",
      "combined_df use 844.44 mb in memory\n",
      "train_df use 373.12 mb in memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/n-surkov/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged_df use 664.55 mb in memory\n",
      "questions_characteristics use 0.67 mb in memory\n",
      "users_characteristics use 3.45 mb in memory\n",
      "combined_df use 1011.36 mb in memory\n",
      "test_df use 441.86 mb in memory\n",
      "last_users_answers use 10.26 mb in memory\n",
      "==== Fold 1 ====\n",
      "Training until validation scores don't improve for 12 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's auc: 0.745307\n",
      "merged_df use 569.50 mb in memory\n",
      "questions_characteristics use 0.62 mb in memory\n",
      "users_characteristics use 3.13 mb in memory\n",
      "combined_df use 844.44 mb in memory\n",
      "train_df use 373.12 mb in memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/n-surkov/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged_df use 664.55 mb in memory\n",
      "questions_characteristics use 0.67 mb in memory\n",
      "users_characteristics use 3.45 mb in memory\n",
      "combined_df use 1011.36 mb in memory\n",
      "test_df use 441.86 mb in memory\n",
      "last_users_answers use 10.26 mb in memory\n",
      "==== Fold 2 ====\n",
      "Training until validation scores don't improve for 12 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's auc: 0.745018\n",
      "CPU times: user 13min 41s, sys: 2min 11s, total: 15min 53s\n",
      "Wall time: 3min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Training Loop\n",
    "counter = 1\n",
    "\n",
    "for train_index, valid_index in skf_split:\n",
    "    train_df, q_chars, u_chars = prepare_train(train_data.iloc[train_index, :], questions_df)\n",
    "    last_answers = get_users_last_answers(train_data.iloc[train_index, :])\n",
    "    valid_df, q_chars, u_chars, last_answers = prepare_test(train_data.iloc[valid_index, :], \n",
    "                                                           last_answers, questions_df, \n",
    "                                                           u_chars, q_chars)\n",
    "    valid_df = (\n",
    "        valid_df.reset_index()\n",
    "        .merge(train_data.iloc[valid_index, :][['row_id', 'answered_correctly']], on='row_id', how='inner')\n",
    "        .set_index('row_id')\n",
    "    )\n",
    "    \n",
    "    print(\"==== Fold {} ====\".format(counter))\n",
    "    \n",
    "    lgbm_train = lgbm.Dataset(data = train_df.drop(columns='answered_correctly').values,\n",
    "                              label = train_df['answered_correctly'].values,\n",
    "                              #feature_name = features_to_keep,\n",
    "                              free_raw_data = False)\n",
    "    \n",
    "    lgbm_valid = lgbm.Dataset(data = valid_df.values,\n",
    "                              label = valid_df['answered_correctly'].values,\n",
    "                              #feature_name = features_to_keep,\n",
    "                              free_raw_data = False)\n",
    "    \n",
    "    lgbm_2 = lgbm.train(params = param, train_set = lgbm_train, valid_sets = [lgbm_valid],\n",
    "                        early_stopping_rounds = 12, num_boost_round=100, verbose_eval=25)\n",
    "    \n",
    "    \n",
    "    # X_valid to predict\n",
    "    oof[valid_index] = lgbm_2.predict(valid_df.drop(columns='answered_correctly').values, \n",
    "                                      num_iteration = lgbm_2.best_iteration)\n",
    "    predictions += lgbm_2.predict(test_df, \n",
    "                                  num_iteration = lgbm_2.best_iteration) / n_splits\n",
    "    \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV ROC: 0.72\n"
     ]
    }
   ],
   "source": [
    "print(\"CV ROC: {:<0.2f}\".format(metrics.roc_auc_score(test_df['answered_correctly'], predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
